Tool Resource Prediction for Genomic Datasets

TODO:
- dont look at the tools that are used the most but instead that have high memory usage. This has more impact on optimization
	- hifiasm, any tool with *spades in name, trinity, rnastar
- try threshold where entries with filesize below lets say 0.05 gb or something like that are filtered out
- plot with date as group
- add tool name when saving training_results
- add timestamp when saving an entry for training_results
- play around with parameters for RF
- label the data with quartals and see if data from specific quartals has higher error rates
- try out some Preproccesors, they help to improve
- use other models as well
- try 3 different seeds and avg over them
- data analysis: for most used tool: generate a graph filesize -> memory
	- also look if there are differences of the data for different time periods (2020, 2021, 2022)
	- memory usage vs time plot

- plot: filesize vs error for mixed_data, valid_data and faulty_data
- plot: filesize vs memory_bytes
- see if training changes if we use 1st quartal 2020, 2nd quartal 2020 etc.


- maybe try to include the category of the tool for the prediction?
- try to get features for the different tools: like number params
- XGB: extra gradient boosting
- try other models:
	- simple models Linear Regressor or SVR
	- Ensemble based or tree based methods
	- e.g. Gradient boosting
	- see on the bottom
- create benchmarks: one low capacity model fitted on data, one high capacity model fitted on data, one model that assigns random values
- train on top 5 tools or more
- use all features and do feature selection



- in future: find out if tool_id is important for prediction
- in future: is it possible to predict runtime?


DONE:
- what is the error for faulty_data compared to valid_data
- ONLY USE SAME VERSION FOR TOOLS!
	- make distinction between versions
- include create_time
- Some entries in the data have filesize 0 but number of files more than 0: --> leave them out
- leave out "Runtime"
- Processed whole dataset again, but got error during it. Probably memory problems. Total number entries now: 19139913
- Filter all entries that have invalid data this includes:
	- data with memory bytes over 1 TB
	- data with memory bytes over 8 * assigned memory


Questions:
- Categorical data: how should I encode the tool name?
- question for later on: for the prediction, do we want to use multiple models for different tools or one model for all tools?

Infos:
- write report 20-30 pages
- bw cloud: https://portal.bw-cloud.org/
- Dataset: https://usegalaxy.eu/u/kumara/h/tool-resource-prediction
- Tool Configurations: https://github.com/usegalaxy-eu/infrastructure-playbook/blob/master/files/galaxy/dynamic_rules/usegalaxy/tool_destinations.yaml
- Chat: https://app.element.io/#/welcome
	- @anupkumar:matrix.org
- Handling bis csv Files: https://www.gigasheet.co/

group tools that have different version or make a distinction between them?
Is there a big variance between versions for a specific tool? Do an analysis on that and find out

papers:
https://ris.uni-paderborn.de/download/16219/16220/ris_preprint.pdf
https://link.springer.com/content/pdf/10.1007/s10723-021-09561-3.pdf
https://upcommons.upc.edu/bitstream/handle/2117/106670/128908.pdf
				

Possible methods:
	- Decision Trees
	- Ensemble Learning (use different models and average them)
		- e.g Random forests or Gradient boosting (better than Random forests according to wikipedia)
	- Neural networks
	
Ideas:
	- do we also want to minimize the time for a tool to complete?