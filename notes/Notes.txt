Tool Resource Prediction for Genomic Datasets

Question:
- should I remove only values of train set or test set as well?
- could we do experiments to initialize jobs with our predicted memory and see what the success rate is?

- what does the job record? The maximum memory usage of a tool

TODO:
- calc R2 value before --> then remove data points which lie outside of 2 standard deviations -> calc R2 again and compare
	- should I remove only values of train set or test set as well?
- Try XGB: https://xgboost.readthedocs.io/en/stable/python/python_api.html
- Create a baseline: use assigned memory from the tool-configuration file
- Hyperparameter Optimization with scikit-learn
	- Grid search
	- Random search
	- both with successive halving
- our task should be able to be given a percentage with which the tool should succeed given the output of our model. Try to find a way to include this
	- Prediction Intervals for Gradient Boosting Regression (https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html)
	- http://contrib.scikit-learn.org/forest-confidence-interval/index.html
- look into the code here to see how the data recording is done: https://github.com/usegalaxy-eu/galaxy/tree/release_22.01_europe/lib/galaxy
- look here: https://bioblend.readthedocs.io/en/latest/api_docs/galaxy/all.html#module-bioblend.galaxy.jobs
- try 3 different seeds and avg over them
- try out some Preproccesors, they help to improve
- use other models as well


- maybe try to include the category of the tool for the prediction?
- try to get features for the different tools: like number params
- XGB: extra gradient boosting
- try other models:
	- simple models Linear Regressor or SVR
	- Ensemble based or tree based methods
	- e.g. Gradient boosting
	- see on the bottom
- create benchmarks: one low capacity model fitted on data, one high capacity model fitted on data, one model that assigns random values


- in future: find out if tool_id is important for prediction
- in future: is it possible to predict runtime?


DONE:
- what is the error for faulty_data compared to valid_data
- ONLY USE SAME VERSION FOR TOOLS!
	- make distinction between versions
- include create_time
- Some entries in the data have filesize 0 but number of files more than 0: --> leave them out
- leave out "Runtime"
- Processed whole dataset again, but got error during it. Probably memory problems. Total number entries now: 19139913
- Filter all entries that have invalid data this includes:
	- data with memory bytes over 1 TB
	- data with memory bytes over 8 * assigned memory


Questions:
- Categorical data: how should I encode the tool name?
- question for later on: for the prediction, do we want to use multiple models for different tools or one model for all tools?

Infos:
- write report 20-30 pages
- ZOOM Link: https://uni-freiburg.zoom.us/j/64335242368?pwd=YmpURjd3Q1YwWHgwYW5Eb2hLeVB6QT09
- bw cloud: https://portal.bw-cloud.org/
- Dataset: https://usegalaxy.eu/u/kumara/h/tool-resources
- Tool Configurations: https://github.com/usegalaxy-eu/infrastructure-playbook/blob/master/files/galaxy/dynamic_rules/usegalaxy/tool_destinations.yaml
- Chat: https://app.element.io/#/welcome
	- @anupkumar:matrix.org
- Handling bis csv Files: https://www.gigasheet.co/
- Old Dataset: https://usegalaxy.eu/u/kumara/h/tool-resource-prediction

group tools that have different version or make a distinction between them?
Is there a big variance between versions for a specific tool? Do an analysis on that and find out

papers:
https://ris.uni-paderborn.de/download/16219/16220/ris_preprint.pdf
https://link.springer.com/content/pdf/10.1007/s10723-021-09561-3.pdf
https://upcommons.upc.edu/bitstream/handle/2117/106670/128908.pdf
				

Possible methods:
	- Decision Trees
	- Ensemble Learning (use different models and average them)
		- e.g Random forests or Gradient boosting (better than Random forests according to wikipedia)
	- Neural networks
	
Ideas:
	- do we also want to minimize the time for a tool to complete?