Tool Resource Prediction for Genomic Datasets

Meeting times:
- Skip next week, instead next meeting 01.06 (Wednesday), as early as possible timewise
- insead of 23.06 lets do 24.06

Question:
- what does the job record? The maximum memory usage of a tool
- start a job for e.g. rna star with high filesize and query and see if it gets recorded correctly

TODO:
- put the plots in the doc
- look into the code here to see how the data recording is done: https://github.com/usegalaxy-eu/galaxy/tree/release_22.01_europe/lib/galaxy
- look here: https://bioblend.readthedocs.io/en/latest/api_docs/galaxy/all.html#module-bioblend.galaxy.jobs
- start a job for e.g. rna star with high filesize and query and see if it gets recorded correctly
- try threshold where entries with filesize below lets say 0.05 gb or something like that are filtered out
	- see if it improves the threshold. If yes lets find out an automatic way to find the threshold
- try 3 different seeds and avg over them
- try out some Preproccesors, they help to improve
- use other models as well
- data analysis: for most used tool: generate a graph filesize -> memory
	- also look if there are differences of the data for different time periods (2020, 2021, 2022)
	- memory usage vs time plot


- maybe try to include the category of the tool for the prediction?
- try to get features for the different tools: like number params
- XGB: extra gradient boosting
- try other models:
	- simple models Linear Regressor or SVR
	- Ensemble based or tree based methods
	- e.g. Gradient boosting
	- see on the bottom
- create benchmarks: one low capacity model fitted on data, one high capacity model fitted on data, one model that assigns random values


- in future: find out if tool_id is important for prediction
- in future: is it possible to predict runtime?


DONE:
- what is the error for faulty_data compared to valid_data
- ONLY USE SAME VERSION FOR TOOLS!
	- make distinction between versions
- include create_time
- Some entries in the data have filesize 0 but number of files more than 0: --> leave them out
- leave out "Runtime"
- Processed whole dataset again, but got error during it. Probably memory problems. Total number entries now: 19139913
- Filter all entries that have invalid data this includes:
	- data with memory bytes over 1 TB
	- data with memory bytes over 8 * assigned memory


Questions:
- Categorical data: how should I encode the tool name?
- question for later on: for the prediction, do we want to use multiple models for different tools or one model for all tools?

Infos:
- write report 20-30 pages
- ZOOM Link: https://uni-freiburg.zoom.us/j/64335242368?pwd=YmpURjd3Q1YwWHgwYW5Eb2hLeVB6QT09
- bw cloud: https://portal.bw-cloud.org/
- Dataset: https://usegalaxy.eu/u/kumara/h/tool-resource-prediction
- Tool Configurations: https://github.com/usegalaxy-eu/infrastructure-playbook/blob/master/files/galaxy/dynamic_rules/usegalaxy/tool_destinations.yaml
- Chat: https://app.element.io/#/welcome
	- @anupkumar:matrix.org
- Handling bis csv Files: https://www.gigasheet.co/

group tools that have different version or make a distinction between them?
Is there a big variance between versions for a specific tool? Do an analysis on that and find out

papers:
https://ris.uni-paderborn.de/download/16219/16220/ris_preprint.pdf
https://link.springer.com/content/pdf/10.1007/s10723-021-09561-3.pdf
https://upcommons.upc.edu/bitstream/handle/2117/106670/128908.pdf
				

Possible methods:
	- Decision Trees
	- Ensemble Learning (use different models and average them)
		- e.g Random forests or Gradient boosting (better than Random forests according to wikipedia)
	- Neural networks
	
Ideas:
	- do we also want to minimize the time for a tool to complete?