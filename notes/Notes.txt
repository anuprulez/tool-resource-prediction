Tool Resource Prediction for Genomic Datasets

Question:
- mention in report: could we do experiments to initialize jobs with our predicted memory and see what the success rate is?
- what does the job record? The maximum memory usage of a tool



- mention:
	- test & train data gets saved
	- save and load model as joblib file
	- continued report. aobut 11 pages
	- log(1+p) transformation

TODO:
- report: when saving the file as .joblib prediction with uncertainty is possible! So uncertainty is available for evaluation as well as training
- log(1+p) transform data to remove skewness of data distribution
	- mention this in report
- write into report similar things as here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6931352/
- create CSV File which lists pearson correlation for all tools
- cross validation?
- put in README how CSViewer can be used
- add link to report in README
- search for TODOs and remove them when finished
- first chapter: motivation
- why not use neural networks? Not many features and there are tools with low number of samples. Our data is not complicated. would be overkill
	--> something for the future?
- try to find similar work. State of the art approaches
- use also tools that are not used as much --> maybe at least 1000-2000
- integrate every tool in one model such that one model can predict for multiple tools (e.g. 5-10 tools)
	- analysis is one model better for multiple tools or is a single model better?
- show data in report: if we have good correlation -> our model works well
					- show cases where correlation is not so good --> model is not so good
- prediction intervals only work if tree is big enough --> how big?
- our task should be able to be given a percentage with which the tool should succeed given the output of our model. Try to find a way to include this
	- Prediction Intervals for Gradient Boosting Regression (https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html)
	- http://contrib.scikit-learn.org/forest-confidence-interval/index.html
- do trainings and compare results with uncertainty and without
- Hyperparameter Optimization with scikit-learn
	- Grid search
	- Random search
	- both with successive halving
- look into the code here to see how the data recording is done: https://github.com/usegalaxy-eu/galaxy/tree/release_22.01_europe/lib/galaxy
- look here: https://bioblend.readthedocs.io/en/latest/api_docs/galaxy/all.html#module-bioblend.galaxy.jobs
- try 3 different seeds and avg over them
- try out some Preproccesors, they help to improve
- use other models as well


- maybe try to include the category of the tool for the prediction?
- try to get features for the different tools: like number params
- XGB: extra gradient boosting
- try other models:
	- simple models Linear Regressor or SVR
	- Ensemble based or tree based methods
	- e.g. Gradient boosting
	- see on the bottom
- create benchmarks: one low capacity model fitted on data, one high capacity model fitted on data, one model that assigns random values


- in future: find out if tool_id is important for prediction
- in future: is it possible to predict runtime?


DONE:
- Create a baseline: use assigned memory from the tool-configuration file
- what is the error for faulty_data compared to valid_data
- ONLY USE SAME VERSION FOR TOOLS!
	- make distinction between versions
- include create_time
- Some entries in the data have filesize 0 but number of files more than 0: --> leave them out
- leave out "Runtime"
- Processed whole dataset again, but got error during it. Probably memory problems. Total number entries now: 19139913
- Filter all entries that have invalid data this includes:
	- data with memory bytes over 1 TB
	- data with memory bytes over 8 * assigned memory


Questions:
- Categorical data: how should I encode the tool name?
- question for later on: for the prediction, do we want to use multiple models for different tools or one model for all tools?

Infos:
- write report 20-30 pages
- presentation: 15-20 mins
- ZOOM Link: https://uni-freiburg.zoom.us/j/64335242368?pwd=YmpURjd3Q1YwWHgwYW5Eb2hLeVB6QT09
- bw cloud: https://portal.bw-cloud.org/
- New dataset: https://usegalaxy.eu/u/kumara/h/tool-resources
- Tool Configurations: https://github.com/usegalaxy-eu/infrastructure-playbook/blob/master/files/galaxy/dynamic_rules/usegalaxy/tool_destinations.yaml
- Chat: https://app.element.io/#/welcome
	- @anupkumar:matrix.org
- Handling big csv Files: https://www.gigasheet.co/ or better https://csviewer.com/
- Old Dataset: https://usegalaxy.eu/u/kumara/h/tool-resource-prediction

group tools that have different version or make a distinction between them?
Is there a big variance between versions for a specific tool? Do an analysis on that and find out

Similar Project:
https://github.com/kxk302/galaxy_job_analysis


older paper about runtime prediction for Galaxy:
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6931352/

example reports:
http://www.bioinf.uni-freiburg.de/Lehre/Theses/P_Ralf_Krauth_Report_Project.pdf
http://www.bioinf.uni-freiburg.de/Lehre/Theses/MA_Ralf_Krauth.pdf


papers:
https://ris.uni-paderborn.de/download/16219/16220/ris_preprint.pdf
https://link.springer.com/content/pdf/10.1007/s10723-021-09561-3.pdf
https://upcommons.upc.edu/bitstream/handle/2117/106670/128908.pdf
				

Possible methods:
	- Decision Trees
	- Ensemble Learning (use different models and average them)
		- e.g Random forests or Gradient boosting (better than Random forests according to wikipedia)
	- Neural networks
	
Ideas:
	- do we also want to minimize the time for a tool to complete?